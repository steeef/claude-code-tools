# Running Claude Code and Codex with Local LLMs

This guide covers running **Claude Code** and **OpenAI Codex CLI** with local
models using [llama.cpp](https://github.com/ggml-org/llama.cpp)'s server:

- **Claude Code** uses the Anthropic-compatible `/v1/messages` endpoint
- **Codex CLI** uses the OpenAI-compatible `/v1/chat/completions` endpoint

## Table of Contents

- [When to Use Local Models](#when-to-use-local-models)
- [How It Works](#how-it-works)
- [Prerequisites](#prerequisites)
- [Shell Function for Claude Code](#shell-function-for-claude-code)
- [Model Commands](#model-commands)
- [Quick Reference](#quick-reference)
- [Usage](#usage)
- [Troubleshooting](#troubleshooting)
- [Using Codex CLI with Local LLMs](#using-codex-cli-with-local-llms)

## When to Use Local Models

These local models (20B-80B parameters) aren't suited for complex coding tasks
where frontier models excel, but they're useful for non-coding tasks like
summarization, answering questions about your private notes, working with
sensitive documents that can't be sent to external APIs, or high-volume tasks
where API costs would add up.

## How It Works

1. **Start llama-server** with a model (see [Model Commands](#model-commands)
   below) - this makes the model available at a local endpoint (e.g., port 8123)
2. **Run Claude Code** pointing to that endpoint using the `cclocal` helper
   function

## Prerequisites

- [llama.cpp](https://github.com/ggml-org/llama.cpp) built and `llama-server`
  available in your PATH
- Sufficient RAM (64GB+ recommended for 30B+ models)
- Models will be downloaded automatically from HuggingFace on first run

## Shell Function for Claude Code

At its simplest, connecting Claude Code to a local model is just one line:

```bash
ANTHROPIC_BASE_URL=http://127.0.0.1:8123 claude
```

The helper function below is just a convenience wrapper for this. Add it to your
`~/.zshrc` or `~/.bashrc`:

```bash
cclocal() {
    local port=8123
    if [[ "$1" =~ ^[0-9]+$ ]]; then
        port="$1"
        shift
    fi
    (
        export ANTHROPIC_BASE_URL="http://127.0.0.1:${port}"
        claude "$@"
    )
}
```

Usage:

```bash
cclocal              # Connect to localhost:8123
cclocal 8124         # Connect to localhost:8124
cclocal 8124 --resume abc123  # With additional claude args
```

## Model Commands

### GPT-OSS-20B (Fast, Good Baseline)

Uses the built-in preset with optimized settings:

```bash
llama-server --gpt-oss-20b-default --port 8123 -a claude-opus-4-5
```

**Performance:** ~17-38 tok/s generation on M1 Max

### Qwen3-30B-A3B

```bash
llama-server -hf unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF \
  --port 8124 \
  -c 131072 \
  -b 32768 \
  -ub 1024 \
  --parallel 1 \
  --jinja \
  --chat-template-file ~/Git/llama.cpp/models/templates/Qwen3-Coder.jinja \
  -a claude-opus-4-5
```

**Performance:** ~15-27 tok/s generation on M1 Max

### Qwen3-Coder-30B-A3B (Recommended)

Uses the built-in preset with Q8_0 quantization (higher quality):

```bash
llama-server --fim-qwen-30b-default --port 8127 -a claude-opus-4-5
```

Downloads `ggml-org/Qwen3-Coder-30B-A3B-Instruct-Q8_0-GGUF` automatically on first
run.

### Qwen3-Next-80B-A3B (Better Long Context)

Newer SOTA model. Slower generation but performance doesn't degrade as much
with long contexts:

```bash
llama-server -hf unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF:Q4_K_XL \
  --port 8126 \
  -c 131072 \
  -b 32768 \
  -ub 1024 \
  --parallel 1 \
  --jinja \
  -a claude-opus-4-5
```

**Performance:** ~5x slower generation than Qwen3-30B-A3B, but better on long
contexts

### Nemotron-3-Nano-30B-A3B (NVIDIA Reasoning Model)

```bash
llama-server -hf unsloth/Nemotron-3-Nano-30B-A3B-GGUF:Q4_K_XL \
  --port 8125 \
  -c 131072 \
  -b 32768 \
  -ub 1024 \
  --parallel 1 \
  --jinja \
  --chat-template-file ~/Git/llama.cpp/models/templates/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16.jinja \
  --temp 0.6 \
  --top-p 0.95 \
  --min-p 0.01 \
  -a claude-opus-4-5
```

**Recommended settings (from NVIDIA):**

- Tool calling: `temp=0.6`, `top_p=0.95`
- Reasoning tasks: `temp=1.0`, `top_p=1.0`

## Quick Reference

| Model | Port | Command |
|-------|------|---------|
| GPT-OSS-20B | 8123 | `llama-server --gpt-oss-20b-default --port 8123 -a claude-opus-4-5` |
| Qwen3-30B-A3B | 8124 | See full command above |
| Nemotron-3-Nano | 8125 | See full command above |
| Qwen3-Next-80B | 8126 | See full command above |
| Qwen3-Coder-30B | 8127 | `llama-server --fim-qwen-30b-default --port 8127 -a claude-opus-4-5` |

## Usage

1. Start the llama-server with your chosen model (first request will be slow
   while model loads)
2. In another terminal, run `cclocal <port>` to start Claude Code
3. Use Claude Code as normal

## Notes

- First request is slow while the model loads into memory (~10-30 seconds
  depending on model size)
- Subsequent requests are fast
- The `-a claude-opus-4-5` flag aliases the model name for compatibility
- The `/v1/messages` endpoint in llama-server handles Anthropic API translation
  automatically
- Each model's chat template handles the model-specific prompt formatting

## Troubleshooting

**"failed to find a memory slot" errors:**

Increase context size (`-c`) or reduce parallel slots (`--parallel 1`). Claude
Code sends large system prompts (~20k+ tokens).

**Slow generation:**

- Increase batch size: `-b 32768`
- Reduce parallel slots: `--parallel 1`
- Check if model is fully loaded in RAM/VRAM

**Model not responding correctly:**

Ensure you're using the correct chat template for your model. The template
handles formatting the Anthropic API messages into the model's expected format.

---

# Using Codex CLI with Local LLMs

[OpenAI Codex CLI](https://github.com/openai/codex) can also use local models via
llama-server's OpenAI-compatible `/v1/chat/completions` endpoint.

## Configuration

Add a local provider to `~/.codex/config.toml`:

```toml
[model_providers.llama-local]
name = "Local LLM via llama.cpp"
base_url = "http://localhost:8123/v1"
wire_api = "chat"
```

For multiple ports (different models), define multiple providers:

```toml
[model_providers.llama-8123]
name = "Local LLM port 8123"
base_url = "http://localhost:8123/v1"
wire_api = "chat"

[model_providers.llama-8124]
name = "Local LLM port 8124"
base_url = "http://localhost:8124/v1"
wire_api = "chat"
```

## Switching Models at Command Line

Use the `--model` flag and `-c` (config) flag to switch models without editing
the TOML file:

```bash
# Use GPT-OSS-20B on port 8123
codex --model gpt-oss-20b -c model_provider=llama-8123

# Use Qwen3-30B on port 8124
codex --model qwen3-30b -c model_provider=llama-8124

# Switch back to OpenAI
codex --model o3
```

You can also override nested config values with dots:

```bash
codex --model gpt-oss-20b \
  -c model_provider=llama-local \
  -c model_providers.llama-local.base_url="http://localhost:8124/v1"
```

## Running llama-server for Codex

Use the same llama-server commands as for Claude Code, but you can omit the
`-a claude-opus-4-5` alias since Codex doesn't check the model name:

```bash
# GPT-OSS-20B
llama-server --gpt-oss-20b-default --port 8123

# Qwen3-Coder-30B
llama-server --fim-qwen-30b-default --port 8127
```

## Notes

- Codex uses the `/v1/chat/completions` endpoint (OpenAI format), not
  `/v1/messages` (Anthropic format)
- Both endpoints are served by llama-server simultaneously
- The same model can serve both Claude Code and Codex at the same time
